import requests
import logging
import time
import random
from bs4 import BeautifulSoup
from utils.mongoUtils import connect_exploit

# PC端的 User-Agent
user_agent_pc = [
    # 谷歌
    'Mozilla/5.0.html (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.html.2171.71 Safari/537.36',
    'Mozilla/5.0.html (X11; Linux x86_64) AppleWebKit/537.11 (KHTML, like Gecko) Chrome/23.0.html.1271.64 Safari/537.11',
    'Mozilla/5.0.html (Windows; U; Windows NT 6.1; en-US) AppleWebKit/534.16 (KHTML, like Gecko) Chrome/10.0.html.648.133 Safari/534.16',
    # 火狐
    'Mozilla/5.0.html (Windows NT 6.1; WOW64; rv:34.0.html) Gecko/20100101 Firefox/34.0.html',
    'Mozilla/5.0.html (X11; U; Linux x86_64; zh-CN; rv:1.9.2.10) Gecko/20100922 Ubuntu/10.10 (maverick) Firefox/3.6.10',
    # opera
    'Mozilla/5.0.html (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.html.2171.95 Safari/537.36 OPR/26.0.html.1656.60',
    # qq浏览器
    'Mozilla/5.0.html (compatible; MSIE 9.0.html; Windows NT 6.1; WOW64; Trident/5.0.html; SLCC2; .NET CLR 2.0.html.50727; .NET CLR 3.5.30729; .NET CLR 3.0.html.30729; Media Center PC 6.0.html; .NET4.0C; .NET4.0E; QQBrowser/7.0.html.3698.400)',
    # 搜狗浏览器
    'Mozilla/5.0.html (Windows NT 5.1) AppleWebKit/535.11 (KHTML, like Gecko) Chrome/17.0.html.963.84 Safari/535.11 SE 2.X MetaSr 1.0.html',
    # 360浏览器
    'Mozilla/5.0.html (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/30.0.html.1599.101 Safari/537.36',
    'Mozilla/5.0.html (Windows NT 6.1; WOW64; Trident/7.0.html; rv:11.0.html) like Gecko',
    # uc浏览器
    'Mozilla/5.0.html (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/38.0.html.2125.122 UBrowser/4.0.html.3214.0.html Safari/537.36',
]

'''截止2021-09-26 exploit-db 共50331条数据'''
coll = connect_exploit()
def exp_spider():
    base = 'https://www.exploit-db.com/exploits/'
    headers = {
        'User-Agent': random.choice(user_agent_pc)
    }
    exp_id = 223
    print()
    while (exp_id <= 50331):
        url = base + str(exp_id)
        res = requests.get(url, headers=headers)
        soup = BeautifulSoup(res.text)
        exp = {}
        exp['title'] = soup.title.string
        for div in soup.find_all('div', class_='col-sm-12 col-md-6 col-lg-3 d-flex align-items-stretch'):
            for h in div.find_all('div', class_='col-6 text-center'):
                if h.h4.get_text().strip() == 'EDB-ID:' :
                    exp['edb_id'] = h.h6.get_text().strip()
                if h.h4.get_text().strip() == 'CVE:' :
                    exp['cve'] = h.h6.get_text().strip()
                if h.h4.get_text().strip() == 'Author:' :
                    exp['author'] = h.h6.get_text().strip()
                if h.h4.get_text().strip() == 'Type:' :
                    exp['type'] = h.h6.get_text().strip()
                if h.h4.get_text().strip() == 'Platform:' :
                    exp['plat'] = h.h6.get_text().strip()
                if h.h4.get_text().strip() == 'Date:' :
                    exp['date'] = h.h6.get_text().strip()
            for s in div.find_all('div', class_='stats h5 text-center'):
                if s.strong.string.strip() == 'EDB Verified:':
                    if s.i['class'] == ['mdi', 'mdi-24px', 'mdi-check']:
                        exp['verified'] = 'Yes'
                    else:
                        exp['verified'] = 'No'
                elif s.strong.string.strip() == 'Exploit:':
                    exp['code'] = base + s.a['href']
                else:
                    if s.find('a'):
                        exp['app'] = base + s.a['href']

        print(exp)
        coll.insert_one(exp)
        if exp_id % 100 == 0:
            time.sleep(1)
        exp_id += 1